"""
BaseScraper - The Smart Exoskeleton with Stealth Mode + Cognitive Engine

Military-grade base class for all spider bots.
Now featuring:
- STEALTH: curl_cffi mimics Chrome 120 TLS fingerprint (bypasses bot detection)
- FALLBACK: httpx for when curl_cffi unavailable
- üåê BROWSER MODE: Playwright fallback for sites requiring real behavior
- üç™ AUTO COOKIES: Pulls fresh session cookies from Redis automatically
- Adaptive throttling (reads X-RateLimit headers)
- Exponential backoff with jitter
- Circuit breaking (pauses after repeated failures)
- Flight Recorder (logs failed requests for Dojo replay)
- üß© CAPTCHA SOLVING: Automatic CAPSOLVER integration (reCAPTCHA, Turnstile, Cloudflare, AWS WAF)
- üß† COGNITIVE ENGINE (2026): LLM parsing, vision verification, smart query mutation

All spiders generated by The Dojo inherit from this class.
"""

import asyncio
import base64
import json
import os
import re
import random
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, Dict, Optional, TypeVar, Union
from enum import Enum

from loguru import logger
from pydantic import BaseModel
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

# CAPTCHA solving
from app.scraping.captcha_solver import (
    get_captcha_solver,
    detect_captcha_in_html,
    is_cloudflare_challenge,
)

# Cookie store for auto-loading session cookies from Redis
try:
    from app.scraping.cookie_store import get_cookie_store, CookieStore
    COOKIE_STORE_AVAILABLE = True
except ImportError:
    COOKIE_STORE_AVAILABLE = False
    logger.warning("‚ö†Ô∏è Cookie store not available")

# Browser mode for full behavior simulation
try:
    from app.scraping.browser_mode import BrowserModeScraper
    BROWSER_MODE_AVAILABLE = True
except ImportError:
    BROWSER_MODE_AVAILABLE = False
    logger.warning("‚ö†Ô∏è Browser mode not available")

# Proxy Configuration (Decodo / Smart Proxies)
# Format: http://user:password@gate.decodo.com:port
# Or use DECODO_API_KEY with default gate
PROXY_URL = os.getenv("PROXY_URL") or os.getenv("ROTATING_PROXY_URL")
DECODO_API_KEY = os.getenv("DECODO_API_KEY")
DECODO_USER = os.getenv("DECODO_USER", "user")  # Default user if not set

def get_proxy_url() -> Optional[str]:
    """Get configured proxy URL for residential rotating proxies"""
    # If explicit PROXY_URL is set, use it directly
    if PROXY_URL:
        return PROXY_URL
    
    # If Decodo API key is set, construct the proxy URL
    if DECODO_API_KEY:
        # Decodo residential proxy format
        # gate.decodo.com:7000 for rotating residential
        return f"http://{DECODO_USER}:{DECODO_API_KEY}@gate.decodo.com:7000"
    
    return None

# Type variable for response models
T = TypeVar('T', bound=BaseModel)

# Coords regex for VLM captcha responses (reused from Chimera captcha_agent logic)
def _parse_captcha_coords(description: str) -> list:
    """Parse 'x y' or 'x,y' pairs from VLM description. Returns [(x,y), ...]."""
    out: list = []
    for m in re.finditer(r"[\[]?\s*(\d+)\s*[,]\s*(\d+)\s*[\]]?|\(?\s*(\d+)\s+(\d+)\s*\)?|(\d+)\s+(\d+)", description or ""):
        g = m.groups()
        x = int(g[0] or g[2] or g[4] or 0)
        y = int(g[1] or g[3] or g[5] or 0)
        if (x, y) not in out:
            out.append((x, y))
    return out

# Try to import curl_cffi for stealth mode
try:
    from curl_cffi.requests import AsyncSession as CurlSession
    STEALTH_AVAILABLE = True
    logger.info("üïµÔ∏è Stealth mode available (curl_cffi)")
except ImportError:
    STEALTH_AVAILABLE = False
    logger.warning("‚ö†Ô∏è curl_cffi not available, using httpx (less stealthy)")

# Fallback to httpx
import httpx

# OpenAI for cognitive capabilities (2026 Tech)
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("‚ö†Ô∏è OpenAI not available - cognitive features disabled")


class CircuitState(Enum):
    """Circuit breaker states"""
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failing, reject requests
    HALF_OPEN = "half_open"  # Testing if service recovered


@dataclass
class RateLimitState:
    """Tracks rate limit status from response headers"""
    limit: int = 100
    remaining: int = 100
    reset_at: float = 0.0
    window_ms: int = 60000
    
    @property
    def is_near_limit(self) -> bool:
        """True if remaining is below 10% of limit"""
        return self.remaining < (self.limit * 0.1)
    
    @property
    def utilization(self) -> float:
        """Current utilization percentage (0.0 - 1.0)"""
        if self.limit == 0:
            return 0.0
        return 1.0 - (self.remaining / self.limit)
    
    def update_from_headers(self, headers: Dict[str, str]) -> None:
        """Parse rate limit headers from response"""
        headers_lower = {k.lower(): v for k, v in headers.items()}
        
        # Standard headers
        if 'x-ratelimit-limit' in headers_lower:
            self.limit = int(headers_lower['x-ratelimit-limit'])
        if 'x-ratelimit-remaining' in headers_lower:
            self.remaining = int(headers_lower['x-ratelimit-remaining'])
        if 'x-ratelimit-reset' in headers_lower:
            self.reset_at = float(headers_lower['x-ratelimit-reset'])
        
        # Alternative header names
        if 'ratelimit-limit' in headers_lower:
            self.limit = int(headers_lower['ratelimit-limit'])
        if 'ratelimit-remaining' in headers_lower:
            self.remaining = int(headers_lower['ratelimit-remaining'])
        if 'ratelimit-reset' in headers_lower:
            self.reset_at = float(headers_lower['ratelimit-reset'])


@dataclass
class CircuitBreaker:
    """Circuit breaker to pause scraping after repeated failures"""
    failure_threshold: int = 10
    recovery_timeout: float = 60.0  # seconds
    half_open_max_calls: int = 3
    
    # State
    failure_count: int = field(default=0, init=False)
    success_count: int = field(default=0, init=False)
    state: CircuitState = field(default=CircuitState.CLOSED, init=False)
    last_failure_time: float = field(default=0.0, init=False)
    half_open_calls: int = field(default=0, init=False)
    
    def record_success(self) -> None:
        """Record a successful request"""
        self.failure_count = 0
        self.success_count += 1
        
        if self.state == CircuitState.HALF_OPEN:
            self.half_open_calls += 1
            if self.half_open_calls >= self.half_open_max_calls:
                logger.info("üü¢ Circuit breaker: HALF_OPEN -> CLOSED (recovered)")
                self.state = CircuitState.CLOSED
                self.half_open_calls = 0
    
    def record_failure(self) -> None:
        """Record a failed request"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            logger.warning(f"üî¥ Circuit breaker: OPEN (failed {self.failure_count} times)")
            self.state = CircuitState.OPEN
    
    def can_proceed(self) -> bool:
        """Check if requests are allowed"""
        if self.state == CircuitState.CLOSED:
            return True
        
        if self.state == CircuitState.OPEN:
            # Check if recovery timeout has passed
            elapsed = time.time() - self.last_failure_time
            if elapsed >= self.recovery_timeout:
                logger.info("üü° Circuit breaker: OPEN -> HALF_OPEN (testing recovery)")
                self.state = CircuitState.HALF_OPEN
                self.half_open_calls = 0
                return True
            return False
        
        # HALF_OPEN - allow limited calls
        return self.half_open_calls < self.half_open_max_calls
    
    def reset(self) -> None:
        """Reset circuit breaker to initial state"""
        self.failure_count = 0
        self.success_count = 0
        self.state = CircuitState.CLOSED
        self.half_open_calls = 0


class RetryableError(Exception):
    """Raised when a request should be retried"""
    pass


class RateLimitError(RetryableError):
    """Raised when rate limited (429)"""
    def __init__(self, retry_after: float = 60.0):
        self.retry_after = retry_after
        super().__init__(f"Rate limited. Retry after {retry_after}s")


class CircuitOpenError(Exception):
    """Raised when circuit breaker is open"""
    pass


class AuthError(Exception):
    """Raised on 401/403 responses"""
    pass


# Default browser-like headers
DEFAULT_HEADERS = {
    "Accept": "application/json, text/plain, */*",
    "Accept-Language": "en-US,en;q=0.9",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Cache-Control": "no-cache",
}

# ---------------------------------------------------------------------------
# Chrome impersonation profiles for curl_cffi (must match installed version)
# ---------------------------------------------------------------------------
# curl_cffi 0.5.x supports only: chrome99, chrome100, chrome101, chrome104,
# chrome107, chrome110. chrome116, chrome119, chrome120, etc. require
# curl_cffi >= 0.6.0 and cause: "impersonate chrome119 is not supported".
#
# Do NOT set IMPERSONATE_PROFILES via environment variables. This module does
# not read IMPERSONATE_PROFILES from os.environ. If it did, env values like
# "chrome119" would cause version mismatch errors. Keep the list below
# strictly within chrome101‚Äìchrome110.
#
# Allowed set for runtime check (chrome101 through chrome110 inclusive).
_IMPERSONATE_ALLOWED = {f"chrome{i}" for i in range(101, 111)}

# Internal list: only 101, 104, 107, 110 (all within 101‚Äì110). Do not add
# chrome99/100 (we restrict to 101‚Äì110) or chrome116+ (curl_cffi 0.5.x fails).
IMPERSONATE_PROFILES = [
    "chrome110",
    "chrome107",
    "chrome104",
    "chrome101",
]

# Hard-coded check: refuse to run with profiles outside chrome101‚Äìchrome110.
_bad = [p for p in IMPERSONATE_PROFILES if p not in _IMPERSONATE_ALLOWED]
if _bad:
    raise RuntimeError(
        f"IMPERSONATE_PROFILES must be chrome101‚Äìchrome110 only (curl_cffi 0.5.x). "
        f"Invalid: {_bad}. Do NOT set IMPERSONATE_PROFILES via env; it causes version mismatch (e.g. chrome116+)."
    )
if not IMPERSONATE_PROFILES:
    raise RuntimeError("IMPERSONATE_PROFILES cannot be empty.")


class BaseScraper(ABC):
    """
    The Smart Exoskeleton with Stealth Mode
    
    Features:
    - üïµÔ∏è STEALTH: Mimics Chrome TLS fingerprint via curl_cffi
    - üîÑ RESILIENCE: Auto-retries on network errors and 429s
    - ‚ö° ADAPTIVE: Reads rate limit headers and adjusts speed
    - üîå CIRCUIT BREAKER: Pauses after 10 consecutive failures
    - üìº FLIGHT RECORDER: Logs failed requests for replay
    - üß© CAPTCHA SOLVING: Automatic CAPSOLVER integration (reCAPTCHA, Turnstile, Cloudflare, AWS WAF)
    
    Usage:
        class LinkedInSpider(BaseScraper):
            async def extract(self, profile_id: str):
                data = await self.get(f"https://linkedin.com/api/profile/{profile_id}")
                return self.parse(data)
    """
    
    def __init__(
        self,
        rate_limit_delay: float = 1.0,
        randomize_delay: bool = True,
        max_retries: int = 5,
        circuit_failure_threshold: int = 10,
        circuit_recovery_timeout: float = 60.0,
        timeout: int = 30,
        headers: Optional[Dict[str, str]] = None,
        proxy: Optional[str] = None,
        stealth: bool = True,
        browser_mode: bool = False,
        platform: Optional[str] = None,
        auto_load_cookies: bool = True,
    ):
        """
        Initialize the scraper with configuration.
        
        Args:
            rate_limit_delay: Base delay between requests (seconds)
            randomize_delay: Add random jitter to delays
            max_retries: Maximum retry attempts for failed requests
            circuit_failure_threshold: Failures before circuit opens
            circuit_recovery_timeout: Seconds before testing recovery
            timeout: Request timeout in seconds
            headers: Additional headers to merge with defaults
            proxy: Proxy URL (e.g., "http://user:pass@proxy:8080")
            stealth: Use curl_cffi for TLS fingerprint stealth (default: True)
            browser_mode: Use Playwright for full behavior simulation (default: False)
            platform: Platform name for auto-loading cookies (linkedin, facebook, etc.)
            auto_load_cookies: Automatically load cookies from Redis (default: True)
        """
        self.rate_limit_delay = rate_limit_delay
        self.randomize_delay = randomize_delay
        self.max_retries = max_retries
        self.timeout = timeout
        # Use provided proxy or auto-detect from environment
        self.proxy = proxy or get_proxy_url()
        self.use_stealth = stealth and STEALTH_AVAILABLE
        
        # Browser mode for sites requiring real browser behavior
        self.browser_mode = browser_mode and BROWSER_MODE_AVAILABLE
        self._browser_scraper: Optional[Any] = None
        
        # Cookie auto-loading from Redis
        self.platform = platform
        self.auto_load_cookies = auto_load_cookies and COOKIE_STORE_AVAILABLE
        self._cookies_loaded = False
        
        if self.proxy:
            logger.info(f"üåê Proxy configured: {self.proxy.split('@')[-1] if '@' in self.proxy else 'custom'}")
        
        if self.browser_mode:
            logger.info("üåê Browser mode enabled (Playwright)")
        
        # Merge custom headers with defaults
        self._headers = {**DEFAULT_HEADERS}
        if headers:
            self._headers.update(headers)
        
        # State tracking
        self.rate_limit = RateLimitState()
        self.circuit = CircuitBreaker(
            failure_threshold=circuit_failure_threshold,
            recovery_timeout=circuit_recovery_timeout,
        )
        
        # Request stats
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.retried_requests = 0
        
        # Session (created lazily)
        self._curl_session: Optional[Any] = None
        self._httpx_client: Optional[httpx.AsyncClient] = None
        
        # Cognitive Engine (2026 Tech) - lazy loaded
        self._ai_client: Optional[Any] = None
        self.use_cognitive_features = os.getenv("ENABLE_COGNITIVE_FEATURES", "true").lower() == "true"
        
        # Logger with context
        self.logger = logger.bind(spider=self.__class__.__name__)
    
    async def _load_cookies_from_redis(self) -> None:
        """
        Load session cookies from Redis for the configured platform.
        
        Uses CookieStore for platforms that require session cookies.
        
        Note: LinkedIn uses RapidAPI (no cookies needed).
        Note: USHA uses JWT token (handled by dnc_scrub module).
        
        Automatically called before first request if platform is set.
        """
        if not self.auto_load_cookies or not self.platform or self._cookies_loaded:
            return
        
        try:
            store = get_cookie_store()
            cookie_header = await store.get_cookie_header(self.platform)
            
            if cookie_header:
                self._headers["Cookie"] = cookie_header
                self.logger.info(f"üç™ Loaded cookies for {self.platform} from Redis")
            else:
                self.logger.debug(f"No cookies found for {self.platform} in Redis")
            
            self._cookies_loaded = True
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Failed to load cookies from Redis: {e}")
            self._cookies_loaded = True  # Don't retry
    
    async def _get_browser_scraper(self):
        """Get or create Playwright browser scraper"""
        if self._browser_scraper is None:
            self._browser_scraper = BrowserModeScraper(
                headless=True,
                stealth=True,
                proxy=self.proxy,
            )
            await self._browser_scraper.start()
            
            # Load cookies into browser if available
            if self.auto_load_cookies and self.platform:
                try:
                    store = get_cookie_store()
                    cookies = await store.get_cookies(self.platform)
                    if cookies:
                        await self._browser_scraper.set_cookies(cookies)
                        self.logger.info(f"üç™ Loaded {len(cookies)} cookies into browser for {self.platform}")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Failed to load cookies into browser: {e}")
        
        return self._browser_scraper
    
    async def _get_session(self) -> Union[Any, httpx.AsyncClient]:
        """Get or create the HTTP session"""
        # Load cookies from Redis if needed
        await self._load_cookies_from_redis()
        
        if self.use_stealth:
            if self._curl_session is None:
                # Pick a random Chrome impersonation profile
                profile = random.choice(IMPERSONATE_PROFILES)
                self.logger.debug(f"üïµÔ∏è Stealth mode: impersonating {profile}")
                
                proxies = None
                if self.proxy:
                    proxies = {"http": self.proxy, "https": self.proxy}
                
                self._curl_session = CurlSession(
                    impersonate=profile,
                    timeout=self.timeout,
                    proxies=proxies,
                    headers=self._headers,
                )
            return self._curl_session
        else:
            if self._httpx_client is None or self._httpx_client.is_closed:
                self._httpx_client = httpx.AsyncClient(
                    headers=self._headers,
                    timeout=httpx.Timeout(self.timeout),
                    follow_redirects=True,
                )
            return self._httpx_client
    
    async def close(self) -> None:
        """Close the HTTP session and browser"""
        if self._curl_session:
            await self._curl_session.close()
            self._curl_session = None
        if self._httpx_client and not self._httpx_client.is_closed:
            await self._httpx_client.aclose()
            self._httpx_client = None
        if self._browser_scraper:
            await self._browser_scraper.close()
            self._browser_scraper = None
    
    async def __aenter__(self):
        """Async context manager entry"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.close()
    
    def _add_jitter(self, delay: float) -> float:
        """Add random jitter to a delay value"""
        if self.randomize_delay:
            jitter = random.uniform(0.5, 1.5)
            return delay * jitter
        return delay
    
    def _calculate_delay(self) -> float:
        """Calculate adaptive delay based on rate limit state"""
        base_delay = self.rate_limit_delay
        
        # If near rate limit, slow down by 50-100%
        if self.rate_limit.is_near_limit:
            slowdown = 1.5 + (self.rate_limit.utilization * 0.5)
            base_delay *= slowdown
            self.logger.debug(f"‚ö†Ô∏è Near rate limit ({self.rate_limit.remaining}/{self.rate_limit.limit}), slowing to {base_delay:.2f}s")
        
        return self._add_jitter(base_delay)
    
    async def _sleep(self, seconds: float) -> None:
        """Sleep with logging"""
        actual_sleep = self._add_jitter(seconds)
        self.logger.debug(f"üí§ Sleeping {actual_sleep:.2f}s")
        await asyncio.sleep(actual_sleep)
    
    def _record_failure(self, url: str, status: int, headers: Dict[str, str], content: str) -> None:
        """Flight Recorder: Dumps failed request details for Dojo replay"""
        snapshot = {
            "url": url,
            "status": status,
            "headers": headers,
            "content_preview": content[:500] if content else "",
            "timestamp": time.time(),
            "spider": self.__class__.__name__,
        }
        self.logger.error(f"üìº Flight Recorder Dump: {json.dumps(snapshot)}")
    
    def _parse_retry_after(self, headers: Dict[str, str], status_code: int) -> float:
        """Parse Retry-After header from response"""
        headers_lower = {k.lower(): v for k, v in headers.items()}
        
        retry_after = headers_lower.get('retry-after')
        if retry_after:
            try:
                return float(retry_after)
            except ValueError:
                return 60.0
        
        # Check X-RateLimit-Reset for Unix timestamp
        reset_at = headers_lower.get('x-ratelimit-reset')
        if reset_at:
            try:
                reset_time = float(reset_at)
                wait_time = reset_time - time.time()
                return max(wait_time, 1.0)
            except ValueError:
                pass
        
        # Default based on status
        return 60.0 if status_code == 429 else 10.0

    def _solve_captcha_image_with_vision(self, image_b64: str) -> list:
        """Tier 2 browser: OpenAI Vision to solve reCAPTCHA image challenge. Returns [(x,y), ...] or []."""
        self._setup_ai()
        if not self._ai_client:
            return []
        try:
            resp = self._ai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "This is a reCAPTCHA image challenge. (1) What must the user select? One short phrase. (2) List the pixel center of each matching tile as x,y. Format: x1,y1 x2,y2 ... . Answer:"},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_b64}"}},
                    ],
                }],
                max_tokens=300,
            )
            text = (resp.choices[0].message.content or "").strip()
            return _parse_captcha_coords(text)
        except Exception as e:
            self.logger.debug("Vision captcha solve: %s", e)
            return []

    async def _try_solve_captcha_in_browser(self, browser: Any, url: str) -> bool:
        """
        2026: Tier 2 VLM (OpenAI Vision) for recaptcha_v2 image, then Tier 3 CapSolver.
        Supports recaptcha v2/v3, hcaptcha, turnstile. Returns True if a solve was applied.
        """
        try:
            _get_html = getattr(browser, "get_html", None)
            html = await (_get_html() if _get_html else browser.page.content())
            info = detect_captcha_in_html(html)
            if not info:
                return False
            ct = info.get("type") or ""
            sk = (info.get("site_key") or "").strip()
            if not sk:
                try:
                    raw = await browser.page.evaluate(
                        "() => { const d = document.querySelector('[data-sitekey]'); return d ? d.getAttribute('data-sitekey') : null; }"
                    )
                    sk = (raw or "").strip() if isinstance(raw, str) else ""
                except Exception:
                    pass

            # Tier 2: OpenAI Vision for recaptcha_v2 image challenge only
            if ct == "recaptcha_v2" and self.use_cognitive_features:
                self._setup_ai()
                if getattr(self, "_ai_client", None):
                    try:
                        frame_el = await browser.page.query_selector("iframe[title*='challenge'], iframe[src*='recaptcha'], iframe[src*='hcaptcha']")
                        fr = await frame_el.content_frame() if frame_el else None
                        if fr:
                            shot = await fr.screenshot()
                            box = await frame_el.bounding_box() if frame_el else None
                            ox = float(box.get("x") or 0) if box else 0.0
                            oy = float(box.get("y") or 0) if box else 0.0
                        else:
                            shot = await browser.page.screenshot()
                            ox, oy = 0.0, 0.0
                        if shot and len(shot) >= 8:
                            b64 = base64.b64encode(shot).decode()
                            loop = asyncio.get_event_loop()
                            coords = await loop.run_in_executor(None, lambda: self._solve_captcha_image_with_vision(b64))
                            if coords:
                                for (x, y) in coords:
                                    await browser.page.mouse.click(float(x) + ox, float(y) + oy)
                                    await asyncio.sleep(0.2 + random.uniform(0, 0.15))
                                self.logger.info("üß© [BROWSER] OpenAI Vision solved recaptcha image")
                                await asyncio.sleep(2)
                                return True
                    except Exception as e:
                        self.logger.debug("Browser VLM captcha: %s", e)

            # Tier 3: CapSolver
            solver = get_captcha_solver()
            if not solver.is_available() or not sk:
                return False
            token = None
            if ct == "recaptcha_v2":
                token = await solver.solve_recaptcha_v2(url, sk, self.proxy)
            elif ct == "recaptcha_v3":
                token = await solver.solve_recaptcha_v3(url, sk, proxy=self.proxy)
            elif ct == "hcaptcha":
                token = await solver.solve_hcaptcha(url, sk, self.proxy)
            elif ct == "turnstile":
                token = await solver.solve_turnstile(url, sk, self.proxy)
            else:
                return False
            if not token:
                return False
            await browser.page.evaluate("""(tok) => {
                const sel = 'textarea[name="g-recaptcha-response"], #g-recaptcha-response, textarea[name="h-captcha-response"], textarea[name="cf-turnstile-response"], input[name="cf-turnstile-response"]';
                const el = document.querySelector(sel);
                if (el) { el.value = tok; if (el.innerHTML !== undefined) el.innerHTML = tok; el.dispatchEvent(new Event('input', { bubbles: true })); }
                const d = document.querySelector('[data-callback]');
                const fn = d ? d.getAttribute('data-callback') : null;
                if (fn && typeof window[fn] === 'function') window[fn](tok);
            }""", token)
            self.logger.info("üß© [BROWSER] Capsolver token injected (%s)", ct)
            await asyncio.sleep(2)
            return True
        except Exception as e:
            self.logger.warning("‚ö†Ô∏è [BROWSER] CAPTCHA solve failed: %s", e)
            return False

    async def _make_browser_request(
        self,
        url: str,
        wait_for_selector: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Make a request using Playwright browser (full behavior simulation).
        2026: Optional warmup (ENABLE_BROWSER_WARMUP=1). If CAPTCHA detected, VLM-first then CapSolver.
        """
        try:
            browser = await self._get_browser_scraper()
            if os.getenv("ENABLE_BROWSER_WARMUP", "").lower() in ("1", "true"):
                try:
                    from urllib.parse import urlparse
                    p = urlparse(url)
                    origin = f"{p.scheme or 'https'}://{p.netloc}/"
                    if origin.rstrip("/") != url.split("?")[0].rstrip("/"):
                        await browser.goto(origin)
                        await asyncio.sleep(2)
                except Exception:
                    pass
            self.logger.info(f"üåê [BROWSER] Navigating to {url}")
            await browser.goto(url)
            if wait_for_selector:
                await browser.wait_for_selector(wait_for_selector)
            _get_html = getattr(browser, "get_html", None)
            html = await (_get_html() if _get_html else browser.page.content())
            if detect_captcha_in_html(html) and (get_captcha_solver().is_available() or (self.use_cognitive_features and os.getenv("OPENAI_API_KEY"))):
                if await self._try_solve_captcha_in_browser(browser, url):
                    await asyncio.sleep(3)
                    html = await (_get_html() if _get_html else browser.page.content())
            self.circuit.record_success()
            return {"text": html, "status": 200}
        except Exception as e:
            self.circuit.record_failure()
            raise
    
    async def _make_request(
        self,
        method: str,
        url: str,
        headers: Optional[Dict[str, str]] = None,
        json_data: Optional[Dict[str, Any]] = None,
        data: Optional[Dict[str, Any]] = None,
        params: Optional[Dict[str, Any]] = None,
        use_browser: Optional[bool] = None,
        wait_for_selector: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Make an HTTP request with full error handling.
        
        Returns parsed JSON or {"text": ..., "status": ...} for non-JSON responses.
        
        Args:
            method: HTTP method (GET, POST)
            url: Target URL
            headers: Additional headers
            json_data: JSON body for POST
            data: Form data for POST
            params: Query parameters
            use_browser: Override browser_mode setting for this request
            wait_for_selector: For browser mode, wait for this selector
        """
        # Determine if we should use browser mode
        should_use_browser = use_browser if use_browser is not None else self.browser_mode
        
        # Browser mode for GET requests
        if should_use_browser and method == "GET":
            return await self._make_browser_request(url, wait_for_selector)
        
        # Check circuit breaker
        if not self.circuit.can_proceed():
            raise CircuitOpenError(
                f"Circuit breaker is OPEN. Wait {self.circuit.recovery_timeout}s for recovery."
            )
        
        # Merge headers
        request_headers = {**self._headers}
        if headers:
            request_headers.update(headers)
        
        session = await self._get_session()
        self.total_requests += 1
        
        # Human-like jitter before request
        await asyncio.sleep(random.uniform(0.5, 2.0))
        
        # Retry loop with exponential backoff
        last_exception = None
        for attempt in range(self.max_retries + 1):
            try:
                self.logger.info(f"üï∏Ô∏è [{method}] {url}")
                
                # Make request (curl_cffi vs httpx have slightly different APIs)
                if self.use_stealth:
                    if method == "GET":
                        response = await session.get(url, headers=request_headers, params=params)
                    else:
                        response = await session.post(url, headers=request_headers, json=json_data, data=data, params=params)
                    
                    status_code = response.status_code
                    resp_headers = dict(response.headers)
                    text = response.text
                else:
                    if method == "GET":
                        response = await session.get(url, headers=request_headers, params=params)
                    else:
                        response = await session.post(url, headers=request_headers, json=json_data, data=data, params=params)
                    
                    status_code = response.status_code
                    resp_headers = dict(response.headers)
                    text = response.text
                
                # Update rate limit state
                self.rate_limit.update_from_headers(resp_headers)
                
                # Handle 429 Rate Limit
                if status_code == 429:
                    retry_after = self._parse_retry_after(resp_headers, status_code)
                    self.logger.warning(f"‚ö†Ô∏è Rate Limit Hit (429). Sleeping {retry_after}s...")
                    await self._sleep(retry_after)
                    self.retried_requests += 1
                    continue  # Retry
                
                # Handle Auth Failures
                if status_code in [401, 403]:
                    self.logger.error(f"‚õî Auth/Access Denied ({status_code}): {url}")
                    self._record_failure(url, status_code, resp_headers, text)
                    raise AuthError(f"Access Denied {status_code}")
                
                # Handle Server Errors (retry)
                if status_code >= 500:
                    self.logger.error(f"üî• Server Error ({status_code})")
                    self._record_failure(url, status_code, resp_headers, text)
                    self.circuit.record_failure()
                    self.retried_requests += 1
                    
                    backoff = min(2 ** attempt, 60)
                    self.logger.warning(f"Retry {attempt + 1}/{self.max_retries}. Waiting {backoff}s")
                    await self._sleep(backoff)
                    continue
                
                # Handle Client Errors - Check for CAPTCHA first
                if status_code >= 400:
                    # Detect and solve CAPTCHA if present
                    captcha_info = None
                    if status_code in [403, 503] or text:
                        captcha_info = detect_captcha_in_html(text) or (
                            {"type": "cloudflare_challenge"} if is_cloudflare_challenge(status_code, text)
                            else None
                        )
                    
                    if captcha_info and get_captcha_solver().is_available():
                        try:
                            self.logger.warning(f"üß© CAPTCHA detected ({captcha_info['type']}) - solving...")
                            solution = await self._solve_captcha(captcha_info, url)
                            
                            # Retry request with CAPTCHA solution
                            if solution:
                                self.logger.info("‚úÖ CAPTCHA solved - retrying request")
                                
                                # Handle Cloudflare cookies (need to inject into session)
                                if captcha_info['type'] == 'cloudflare_challenge' and isinstance(solution, dict):
                                    cookies = solution.get('cookie', {})
                                    if cookies:
                                        # Update session cookies if using httpx
                                        # curl_cffi handles cookies automatically via session
                                        if not self.use_stealth and self._httpx_client:
                                            for cookie_name, cookie_value in cookies.items():
                                                self._httpx_client.cookies.set(cookie_name, cookie_value)
                                        self.logger.debug(f"üç™ Set {len(cookies)} Cloudflare cookies")
                                
                                # Wait a bit for solution to propagate
                                await asyncio.sleep(1)
                                continue  # Retry loop
                        except Exception as e:
                            self.logger.error(f"‚ùå CAPTCHA solving failed: {e}")
                            # Continue to normal error handling
                    
                    # No CAPTCHA or solving failed - normal error handling
                    self._record_failure(url, status_code, resp_headers, text)
                    raise Exception(f"Client Error {status_code}: {text[:200]}")
                
                # Success!
                self.circuit.record_success()
                self.successful_requests += 1
                
                # Adaptive delay before next request
                delay = self._calculate_delay()
                if delay > 0:
                    await self._sleep(delay)
                
                # Parse JSON or return text
                try:
                    return json.loads(text) if text else {}
                except json.JSONDecodeError:
                    return {"text": text, "status": status_code}
                
            except AuthError:
                raise
            except CircuitOpenError:
                raise
            except Exception as e:
                last_exception = e
                self.circuit.record_failure()
                self.retried_requests += 1
                
                backoff = min(2 ** attempt, 60)
                self.logger.warning(f"‚ùå Error: {e}. Retry {attempt + 1}/{self.max_retries}. Waiting {backoff}s")
                await self._sleep(backoff)
        
        # All retries exhausted
        self.failed_requests += 1
        self.circuit.record_failure()
        self.logger.error(f"üíÄ All {self.max_retries} retries failed for {url}")
        raise last_exception or Exception("Request failed after all retries")
    
    async def get(
        self,
        url: str,
        headers: Optional[Dict[str, str]] = None,
        params: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Make a GET request with stealth and full error handling.
        
        Args:
            url: The URL to fetch
            headers: Additional headers (merged with defaults)
            params: Query parameters
            
        Returns:
            Parsed JSON response
        """
        return await self._make_request("GET", url, headers=headers, params=params)
    
    async def post(
        self,
        url: str,
        headers: Optional[Dict[str, str]] = None,
        json: Optional[Dict[str, Any]] = None,
        data: Optional[Dict[str, Any]] = None,
        params: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Make a POST request with stealth and full error handling.
        
        Args:
            url: The URL to post to
            headers: Additional headers (merged with defaults)
            json: JSON body (automatically serialized)
            data: Form data
            params: Query parameters
            
        Returns:
            Parsed JSON response
        """
        return await self._make_request("POST", url, headers=headers, json_data=json, data=data, params=params)
    
    # Aliases for consistency
    async def get_json(self, url: str, **kwargs) -> Dict[str, Any]:
        """Alias for get()"""
        return await self.get(url, **kwargs)
    
    async def post_json(self, url: str, **kwargs) -> Dict[str, Any]:
        """Alias for post()"""
        return await self.post(url, **kwargs)
    
    def get_stats(self) -> Dict[str, Any]:
        """Get scraper statistics"""
        return {
            "spider": self.__class__.__name__,
            "stealth_mode": self.use_stealth,
            "browser_mode": self.browser_mode,
            "platform": self.platform,
            "cookies_loaded": self._cookies_loaded,
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "retried_requests": self.retried_requests,
            "success_rate": self.successful_requests / max(self.total_requests, 1),
            "rate_limit": {
                "limit": self.rate_limit.limit,
                "remaining": self.rate_limit.remaining,
                "utilization": self.rate_limit.utilization,
            },
            "circuit_breaker": {
                "state": self.circuit.state.value,
                "failure_count": self.circuit.failure_count,
            },
        }
    
    async def enable_browser_mode(self) -> bool:
        """
        Switch to browser mode at runtime.
        
        Use when HTTP requests are being blocked but browser would work.
        
        Returns:
            True if browser mode enabled successfully
        """
        if not BROWSER_MODE_AVAILABLE:
            self.logger.error("‚ùå Browser mode not available (Playwright not installed)")
            return False
        
        self.browser_mode = True
        self.logger.info("üåê Switched to browser mode (Playwright)")
        return True
    
    async def get_with_browser(
        self,
        url: str,
        wait_for_selector: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Force browser mode for this specific GET request.
        
        Use when you know HTTP won't work but don't want to switch globally.
        
        Args:
            url: Target URL
            wait_for_selector: CSS selector to wait for
            
        Returns:
            {"text": page_html, "status": 200}
        """
        return await self._make_request(
            "GET", 
            url, 
            use_browser=True, 
            wait_for_selector=wait_for_selector
        )
    
    def _setup_ai(self) -> None:
        """
        Lazy load OpenAI client for cognitive tasks.
        
        2026 Tech: Only initializes when needed to avoid unnecessary API calls.
        """
        if not self.use_cognitive_features:
            return
        
        if not OPENAI_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è OpenAI not available - cognitive features disabled")
            return
        
        if self._ai_client is None:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                self.logger.warning("‚ö†Ô∏è OPENAI_API_KEY not set - cognitive features disabled")
                return
            
            self._ai_client = OpenAI(api_key=api_key)
            self.logger.debug("üß† Cognitive Engine initialized")
    
    async def parse_with_llm(
        self, 
        html_snippet: str, 
        schema_description: str,
        model: str = "gpt-4o-mini"
    ) -> Dict[str, Any]:
        """
        2026 TECH: Universal LLM Parser - No More Fragile Selectors.
        
        Uses AI to extract data from obfuscated/messy HTML.
        Bypasses the need for fragile CSS selectors that break when sites redesign.
        
        Args:
            html_snippet: HTML content to parse (will be truncated to 15000 chars)
            schema_description: Description of fields to extract (e.g., "Phone Numbers (list), Age (int), Current Address (string)")
            model: OpenAI model to use (default: gpt-4o-mini - fast and cheap)
            
        Returns:
            Dict with extracted fields, or empty dict on failure
            
        Example:
            data = await self.parse_with_llm(
                html_card,
                "Phone Numbers (list), Age (int), Address (string)"
            )
            # Returns: {"Phone Numbers": ["+1234567890"], "Age": 45, "Address": "123 Main St"}
        """
        self._setup_ai()
        
        if not self._ai_client:
            self.logger.warning("‚ö†Ô∏è AI client not available - skipping LLM parse")
            return {}
        
        # Truncate HTML to save tokens (15000 chars is usually enough for a result card)
        html_truncated = html_snippet[:15000]
        if len(html_snippet) > 15000:
            self.logger.debug(f"üìù Truncated HTML from {len(html_snippet)} to 15000 chars")
        
        prompt = f"""You are a data extraction engine for web scraping.

Extract the following fields from this HTML snippet:
{schema_description}

HTML:
```html
{html_truncated}
```

Return ONLY valid JSON. If a field is missing or not found, use null.
Do not include any explanation or markdown formatting - just the JSON object."""

        try:
            self.logger.info(f"üß† AI Parsing HTML for: {schema_description}")
            
            response = self._ai_client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.1,  # Low temperature for consistent extraction
            )
            
            result = json.loads(response.choices[0].message.content)
            self.logger.success(f"‚úÖ AI extracted {len(result)} fields")
            return result
            
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå AI returned invalid JSON: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå AI Parsing Failed: {e}")
            return {}
    
    async def verify_page_content(
        self, 
        page_text: str, 
        success_keywords: list,
        use_vision: bool = False,
        screenshot_path: Optional[str] = None
    ) -> bool:
        """
        2026 TECH: Vision-Based Verification - Detects Soft Blocks.
        
        Checks if the page actually loaded the content we want,
        or if it's a soft-block/empty result disguised as a 200 OK.
        
        Args:
            page_text: Page HTML or text content
            success_keywords: List of keywords that indicate success (e.g., ["phone", "address"])
            use_vision: If True, use vision model to analyze screenshot (more accurate)
            screenshot_path: Path to screenshot if using vision mode
            
        Returns:
            True if page contains expected content, False if it's a block/empty page
        """
        # Simple keyword check first (fast and cheap)
        page_lower = page_text.lower()
        
        # Check for obvious blocks
        block_indicators = [
            "access denied",
            "blocked",
            "forbidden",
            "cloudflare",
            "checking your browser",
            "please enable javascript",
            "captcha",
            "rate limit",
            "too many requests",
        ]
        
        for indicator in block_indicators:
            if indicator in page_lower:
                self.logger.warning(f"üõ°Ô∏è Block indicator detected: {indicator}")
                return False
        
        # Check for success keywords
        found_keywords = [kw for kw in success_keywords if kw.lower() in page_lower]
        if found_keywords:
            self.logger.debug(f"‚úÖ Success keywords found: {found_keywords}")
            return True
        
        # 2026 Upgrade: Vision-based verification (if enabled and screenshot available)
        if use_vision and screenshot_path:
            return await self._verify_with_vision(screenshot_path, success_keywords)
        
        # If no success keywords found, likely empty/blocked
        self.logger.warning("‚ö†Ô∏è No success keywords found - page may be empty or blocked")
        return False
    
    async def _verify_with_vision(
        self, 
        screenshot_path: str, 
        success_keywords: list
    ) -> bool:
        """
        Use vision model to verify page content.
        
        More accurate than text-based checks for detecting soft blocks.
        """
        self._setup_ai()
        
        if not self._ai_client:
            return False
        
        try:
            from pathlib import Path
            
            if not Path(screenshot_path).exists():
                self.logger.warning(f"‚ö†Ô∏è Screenshot not found: {screenshot_path}")
                return False
            
            import base64
            
            with open(screenshot_path, "rb") as img_file:
                self.logger.info("üëÅÔ∏è Using vision model to verify page content...")
                
                # Encode image to base64
                image_data = base64.b64encode(img_file.read()).decode('utf-8')
                
                response = self._ai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": f"""Analyze this screenshot of a web page.

Is this a real results page with actual data, or is it:
- A block/access denied page?
- An empty "no results found" page?
- A CAPTCHA/challenge page?

Look for these indicators of success: {', '.join(success_keywords)}

Respond with JSON:
{{"is_valid": true/false, "reason": "explanation"}}"""
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{image_data}"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=150,
                )
                
                result = json.loads(response.choices[0].message.content)
                is_valid = result.get("is_valid", False)
                reason = result.get("reason", "")
                
                if is_valid:
                    self.logger.success(f"‚úÖ Vision verification passed: {reason}")
                else:
                    self.logger.warning(f"üõ°Ô∏è Vision verification failed: {reason}")
                
                return is_valid
                
        except Exception as e:
            self.logger.error(f"‚ùå Vision verification error: {e}")
            return False
    
    def generate_query_mutations(
        self, 
        first_name: str, 
        last_name: str, 
        city: str, 
        state: str
    ) -> list:
        """
        2026 TECH: Smart Query Mutation - Fuzzy Logic.
        
        Generates variations of search queries to handle:
        - Nicknames (John -> Jonathan)
        - Location variations (Naples -> Collier County)
        - Common misspellings
        
        Args:
            first_name: First name
            last_name: Last name
            city: City name
            state: State abbreviation
            
        Returns:
            List of (first_name, last_name, city, state) tuples to try
        """
        mutations = []
        
        # Base query (exact match)
        mutations.append((first_name, last_name, city, state))
        
        # Common nickname mappings
        nickname_map = {
            "john": ["jonathan", "jon", "johnny"],
            "jim": ["james", "jimmy"],
            "bob": ["robert", "bobby"],
            "bill": ["william", "billy"],
            "mike": ["michael", "mikey"],
            "dave": ["david", "davey"],
            "chris": ["christopher", "christian"],
            "tom": ["thomas", "tommy"],
            "dan": ["daniel", "danny"],
            "joe": ["joseph", "joey"],
        }
        
        first_lower = first_name.lower()
        if first_lower in nickname_map:
            for nickname in nickname_map[first_lower]:
                mutations.append((nickname.capitalize(), last_name, city, state))
        
        # Location variations (common patterns)
        # Note: This is a simple heuristic - could be enhanced with geocoding API
        location_variations = []
        
        # Try without city (broader search)
        mutations.append((first_name, last_name, "", state))
        
        # Try with "County" suffix
        if "county" not in city.lower():
            mutations.append((first_name, last_name, f"{city} County", state))
        
        self.logger.debug(f"üîÑ Generated {len(mutations)} query mutations")
        return mutations
    
    async def _solve_captcha(self, captcha_info: Dict[str, Any], website_url: str) -> Optional[Any]:
        """
        Solve CAPTCHA using CAPSOLVER
        
        Args:
            captcha_info: Dict with 'type' and optionally 'site_key'
            website_url: URL where CAPTCHA was encountered
            
        Returns:
            Solution token/cookies or None
        """
        solver = get_captcha_solver()
        if not solver.is_available():
            return None
        
        captcha_type = captcha_info.get("type", "")
        site_key = captcha_info.get("site_key", "")
        
        try:
            if captcha_type == "recaptcha_v2":
                return await solver.solve_recaptcha_v2(website_url, site_key, proxy=self.proxy)
            
            elif captcha_type == "recaptcha_v3":
                return await solver.solve_recaptcha_v3(website_url, site_key, proxy=self.proxy)
            
            elif captcha_type == "turnstile":
                return await solver.solve_turnstile(website_url, site_key, proxy=self.proxy)
            
            elif captcha_type == "cloudflare_challenge":
                result = await solver.solve_cloudflare_challenge(website_url, proxy=self.proxy)
                # Set cookies in session for subsequent requests
                if isinstance(result, dict) and "cookie" in result:
                    # Cookies will be set automatically by curl_cffi/httpx if we use the same session
                    return result
                return result
            
            elif captcha_type == "aws_waf":
                return await solver.solve_aws_waf(website_url, proxy=self.proxy)
            
            else:
                self.logger.warning(f"Unsupported CAPTCHA type: {captcha_type}")
                return None
                
        except Exception as e:
            self.logger.error(f"CAPTCHA solving error: {e}")
            return None
    
    @abstractmethod
    async def extract(self, **kwargs) -> Any:
        """
        Main extraction method - override in subclass.
        
        This is where you implement the actual scraping logic.
        Use self.get() and self.post() for requests.
        
        Example:
            async def extract(self, profile_id: str):
                data = await self.get(f"https://api.example.com/profile/{profile_id}")
                return MyModel(**data)
        """
        pass
    
    async def run(self, **kwargs) -> Any:
        """
        Execute the spider with proper setup/teardown.
        
        Args:
            **kwargs: Passed to extract()
            
        Returns:
            Result from extract()
        """
        mode = "STEALTH" if self.use_stealth else "STANDARD"
        self.logger.info(f"üöÄ Starting {self.__class__.__name__} [{mode}]")
        start_time = time.time()
        
        try:
            async with self:
                result = await self.extract(**kwargs)
                
            duration = time.time() - start_time
            stats = self.get_stats()
            self.logger.info(
                f"‚úÖ Completed in {duration:.2f}s. "
                f"Requests: {stats['successful_requests']}/{stats['total_requests']} "
                f"(retried: {stats['retried_requests']})"
            )
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.error(f"üíÄ Failed after {duration:.2f}s: {e}")
            raise
